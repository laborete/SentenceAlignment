{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals \n",
    "from bosonnlp import BosonNLP\n",
    "nlp = BosonNLP('Yy7Tpb83.27824.Zdrfgp6JtKTf')\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "Porter = nltk.stem.PorterStemmer()\n",
    "import pickle\n",
    "from langconv import Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename,method='utf-8'):\n",
    "    rawlist = []\n",
    "    with open(filename,'r+', encoding=method) as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        for row in rows:\n",
    "            rawlist.append(row)\n",
    "    return rawlist\n",
    "\n",
    "def writefile(filename,inputlist,method='utf-8'):\n",
    "    with open(filename,'a+', encoding=method) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        for row in inputlist:\n",
    "            writer.writerow(row)\n",
    "def regex(a):\n",
    "    reg = []\n",
    "    a = re.sub('[,、：；！？，。－\"\\']','#',a)\n",
    "    a = re.sub('[「」(){}:;!?]','',a)\n",
    "    a = re.sub('\\n','',a)\n",
    "    #a = re.sub('[(),{}\\u3000\\*\\|=-\\[\\]\\n.]','',a)\n",
    "    a = re.sub(u\"\\\\（.*?\\\\）|\\\\『.*?』|\\\\「.*?」|\\\\〔.*?〕|\\\\[.*?]|\\\\〈.*?〉\", '', a)\n",
    "    reg = a.split('#')\n",
    "    return reg\n",
    "\n",
    "def no_English(a):\n",
    "    a = re.sub('[a-zA-Z]','X',a)\n",
    "    if 'X' in a:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def to_mandarin(inputstr=''):\n",
    "    output = Converter('zh-hant').convert(inputstr)\n",
    "    return(output)\n",
    "def sentencelize(inputstr=''):\n",
    "    output = regex(to_mandarin(inputstr))\n",
    "    for i in range(0,len(output)-1):\n",
    "        if len(output[i])<=3:\n",
    "            output[i+1] = output[i] + output[i+1]\n",
    "    return output\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8704"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Paper_Dmpairs_ENG.pickle','rb') as f:\n",
    "    AiritiSource = pickle.load(f)\n",
    "AiritiRaw = []\n",
    "for doc in AiritiSource:\n",
    "    for pair in doc:\n",
    "        AiritiRaw.append(pair)\n",
    "len(AiritiRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 1 sentences\n",
      "finish 101 sentences\n",
      "finish 201 sentences\n",
      "finish 301 sentences\n",
      "finish 401 sentences\n",
      "finish 501 sentences\n",
      "finish 601 sentences\n",
      "finish 701 sentences\n",
      "finish 801 sentences\n",
      "finish 901 sentences\n",
      "finish 1001 sentences\n",
      "finish 1101 sentences\n",
      "finish 1201 sentences\n",
      "finish 1301 sentences\n",
      "finish 1401 sentences\n",
      "finish 1501 sentences\n",
      "finish 1601 sentences\n",
      "finish 1701 sentences\n",
      "finish 1801 sentences\n",
      "finish 1901 sentences\n",
      "finish 2001 sentences\n",
      "finish 2101 sentences\n",
      "finish 2201 sentences\n",
      "finish 2301 sentences\n",
      "finish 2401 sentences\n",
      "finish 2501 sentences\n",
      "finish 2601 sentences\n",
      "finish 2701 sentences\n",
      "finish 2801 sentences\n",
      "finish 2901 sentences\n",
      "finish 3001 sentences\n",
      "finish 3101 sentences\n",
      "finish 3201 sentences\n",
      "finish 3301 sentences\n",
      "finish 3401 sentences\n",
      "finish 3501 sentences\n",
      "finish 3601 sentences\n",
      "finish 3701 sentences\n",
      "finish 3801 sentences\n",
      "finish 3901 sentences\n",
      "finish 4001 sentences\n",
      "finish 4101 sentences\n",
      "finish 4201 sentences\n",
      "finish 4301 sentences\n",
      "finish 4401 sentences\n",
      "finish 4501 sentences\n",
      "finish 4601 sentences\n",
      "finish 4701 sentences\n",
      "finish 4801 sentences\n",
      "finish 4901 sentences\n",
      "finish 5001 sentences\n",
      "finish 5101 sentences\n",
      "finish 5201 sentences\n",
      "finish 5301 sentences\n",
      "finish 5401 sentences\n",
      "finish 5501 sentences\n",
      "finish 5601 sentences\n",
      "finish 5701 sentences\n",
      "finish 5801 sentences\n",
      "finish 5901 sentences\n",
      "finish 6001 sentences\n",
      "finish 6101 sentences\n",
      "finish 6201 sentences\n",
      "finish 6301 sentences\n",
      "finish 6401 sentences\n",
      "finish 6501 sentences\n",
      "finish 6601 sentences\n",
      "finish 6701 sentences\n",
      "finish 6801 sentences\n",
      "finish 6901 sentences\n",
      "finish 7001 sentences\n",
      "finish 7101 sentences\n",
      "finish 7201 sentences\n",
      "finish 7301 sentences\n",
      "finish 7401 sentences\n",
      "finish 7501 sentences\n",
      "finish 7601 sentences\n",
      "finish 7701 sentences\n",
      "finish 7801 sentences\n",
      "finish 7901 sentences\n",
      "finish 8001 sentences\n",
      "finish 8101 sentences\n",
      "finish 8201 sentences\n",
      "finish 8301 sentences\n",
      "finish 8401 sentences\n",
      "finish 8501 sentences\n",
      "finish 8601 sentences\n",
      "finish 8701 sentences\n"
     ]
    }
   ],
   "source": [
    "cboson = []\n",
    "tag_boson = []\n",
    "tmp = ''\n",
    "start = 0\n",
    "for i in range(0,len(AiritiRaw)):\n",
    "    try:\n",
    "        result = nlp.tag(AiritiRaw[i][0])\n",
    "        for word in result[0]['word']:\n",
    "            tmp = tmp+word+'#'\n",
    "        cboson.append(tmp)\n",
    "        tmp = ''\n",
    "        for tag in result[0]['tag']:\n",
    "            tmp = tmp+tag+'#'\n",
    "        tag_boson.append(tmp)\n",
    "        tmp = ''\n",
    "    except:\n",
    "        print('summary',i,'does not work.')\n",
    "    start = start+1\n",
    "    if(i%100==0):\n",
    "        print('finish',start,'sentences')\n",
    "        #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect = []\n",
    "for j in range(0,len(AiritiRaw)):\n",
    "    perfect.append([AiritiRaw[j][0],AiritiRaw[j][1],cboson[j],tag_boson[j]])\n",
    "#wiki_chinese\n",
    "with open('Airiti.pickle','wb') as f:\n",
    "    pickle.dump(perfect,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('Airiti.pickle','rb') as f:\n",
    "#    perfect = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['近年來雖然偵測手勢的技術已經被充分的探索',\n",
       " ' While high-resolution and miniature gesture sensing technology has been widely explored the interaction space is still limited due to the nature of low resolution human proprioceptive sense',\n",
       " '近年來#雖然#偵#測#手勢#的#技術#已經#被#充分#的#探索#',\n",
       " 'dl#c#v#v#n#ude#n#d#pbei#a#ude#n#']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfect[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#import ssl\n",
    "#try:\n",
    "#    _create_unverified_https_context = ssl._create_unverified_context\n",
    "#except AttributeError:\n",
    "#    pass\n",
    "#else:\n",
    "#    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "eng = []\n",
    "for x in range(0,len(perfect)):\n",
    "    if x%1000==0:\n",
    "        print(x)\n",
    "    tmpstr = '<bos>/<sos>'\n",
    "    words = nltk.word_tokenize(perfect[x][1])\n",
    "    word_tag = nltk.pos_tag(words)\n",
    "    for i in range(0,len(word_tag)):\n",
    "        tmpstr = tmpstr+' '+word_tag[i][0]+'/'+word_tag[i][1]\n",
    "    tmpstr = tmpstr+' <eos>/<sos>'\n",
    "    eng.append(tmpstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi = []\n",
    "for x in range(0,len(perfect)):\n",
    "    a = perfect[x][2].split('#')\n",
    "    b = perfect[x][3].split('#')\n",
    "    if(len(a)-len(b))==0:\n",
    "        tmpstr = '<bos>/<sos>'\n",
    "        for i in range(0,len(a)-1):\n",
    "            tmpstr = tmpstr+' '+a[i]+'/'+b[i]\n",
    "        tmpstr = tmpstr+' <eos>/<sos>'\n",
    "    else:\n",
    "        tmpstr = '<bos>/<sos>''<eos>/<sos>'\n",
    "        print(x,'Wtf')\n",
    "    chi.append(tmpstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0415英文直接對應文本.txt', 'w+') as f:\n",
    "    for item in eng:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('0415中文直接對應文本.txt', 'w+') as f:\n",
    "    for item in chi:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "realc = readfile('0414中文直接對應文本.txt')\n",
    "reale = readfile('0414英文直接對應文本.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>/<sos> 任何人/r 都/d 可以/v 透過/v 社群/n 網站/n 傳遞/v 訊息/n 這/r 也/d 造成/v 假/a 新聞/n 的/ude 問題/n 變/v 得/ude 越來越/d 嚴重/ad 想/v 要/v 解決/v 假/a 新聞/n 的/ude 問題/n <eos>/<sos>']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realc[115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>/<sos> Recently/RB the/DT fake/JJ news/NN problem/NN becomes/VBZ more/JJR and/CC more/RBR serious/JJ because/IN anyone/NN can/MD tweet/VB or/CC post/VB anything/NN with/IN the/DT click/NN of/IN a/DT mouse/NN <eos>/<sos>']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reale[115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
