{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- CONSTANTS ----------------------------------------------------------------+\n",
    "\n",
    "class word2vec():\n",
    "    def __init__ (self):\n",
    "        self.n = settings['n']\n",
    "        self.eta = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']\n",
    "        self.window = settings['window_size']\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # GENERATE TRAINING DATA\n",
    "    def generate_training_data(self, settings, corpus):\n",
    "\n",
    "        # GENERATE WORD COUNTS\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        self.v_count = len(word_counts.keys())\n",
    "\n",
    "        # GENERATE LOOKUP DICTIONARIES\n",
    "        self.words_list = sorted(list(word_counts.keys()),reverse=False)\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "\n",
    "        training_data = []\n",
    "        # CYCLE THROUGH EACH SENTENCE IN CORPUS\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "\n",
    "            # CYCLE THROUGH EACH WORD IN SENTENCE\n",
    "            for i, word in enumerate(sentence):\n",
    "                \n",
    "                #w_target  = sentence[i]\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "\n",
    "                # CYCLE THROUGH CONTEXT WINDOW\n",
    "                w_context = []\n",
    "                for j in range(i-self.window, i+self.window+1):\n",
    "                    if j!=i and j<=sent_len-1 and j>=0:\n",
    "                        w_context.append(self.word2onehot(sentence[j]))\n",
    "                training_data.append([w_target, w_context])\n",
    "        return np.array(training_data)\n",
    "\n",
    "\n",
    "    # SOFTMAX ACTIVATION FUNCTION\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "    # CONVERT WORD TO ONE HOT ENCODING\n",
    "    def word2onehot(self, word):\n",
    "        word_vec = [0 for i in range(0, self.v_count)]\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "\n",
    "\n",
    "    # FORWARD PASS\n",
    "    def forward_pass(self, x):\n",
    "        h = np.dot(self.w1.T, x)\n",
    "        u = np.dot(self.w2.T, h)\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c, h, u\n",
    "                \n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    def backprop(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)  \n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\n",
    "        # UPDATE WEIGHTS\n",
    "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TRAIN W2V model\n",
    "    def train(self, training_data):\n",
    "        # INITIALIZE WEIGHT MATRICES\n",
    "        self.w1 = np.random.uniform(-0.8, 0.8, (self.v_count, self.n))     # embedding matrix\n",
    "        self.w2 = np.random.uniform(-0.8, 0.8, (self.n, self.v_count))     # context matrix\n",
    "        \n",
    "        # CYCLE THROUGH EACH EPOCH\n",
    "        for i in range(0, self.epochs):\n",
    "\n",
    "            self.loss = 0\n",
    "\n",
    "            # CYCLE THROUGH EACH TRAINING SAMPLE\n",
    "            for w_t, w_c in training_data:\n",
    "\n",
    "                # FORWARD PASS\n",
    "                y_pred, h, u = self.forward_pass(w_t)\n",
    "                \n",
    "                # CALCULATE ERROR\n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "\n",
    "                # BACKPROPAGATION\n",
    "                self.backprop(EI, h, w_t)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "                #self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))\n",
    "                \n",
    "            print('EPOCH:',i, 'LOSS:', self.loss)\n",
    "        pass\n",
    "\n",
    "\n",
    "    # input a word, returns a vector (if available)\n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return v_w\n",
    "\n",
    "\n",
    "    # input a vector, returns nearest word(s)\n",
    "    def vec_sim(self, vec, top_n):\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(vec, v_w2)\n",
    "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda word,sim:sim , reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print(word,sim)\n",
    "            \n",
    "        pass\n",
    "\n",
    "    # input word, returns top [n] most similar words\n",
    "    def word_sim(self, word, top_n):\n",
    "        \n",
    "        w1_index = self.word_index[word]\n",
    "        v_w1 = self.w1[w1_index]\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda word,sim:sim, reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print(word,sim)\n",
    "            \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25695"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('airiti_english_corpus.pickle','rb') as f:\n",
    "    en_data = pickle.load(f)\n",
    "with open('airiti_chinese_corpus.pickle','rb') as f:\n",
    "    zh_data = pickle.load(f)\n",
    "len(zh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('airiti_chinese_corpus.pickle','wb') as f:\n",
    "    pickle.dump(zh_data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文或英文，選一個訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data_dict = dict()\n",
    "source = []\n",
    "for i in range(0,len(en_data)):\n",
    "    for j in range(0,len(en_data[i])):\n",
    "        for k in range(0,len(en_data[i][j])):\n",
    "            if not en_data[i][j][k] in en_data_dict:\n",
    "                en_data_dict[en_data[i][j][k]]=1\n",
    "            else:\n",
    "                en_data_dict[en_data[i][j][k]] = en_data_dict[en_data[i][j][k]]+1\n",
    "            en_data[i][j][k] = en_data[i][j][k].lower()\n",
    "        source.append(en_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_data_dict = dict()\n",
    "source = []\n",
    "for i in range(0,len(zh_data)):\n",
    "    for j in range(0,len(zh_data[i])):\n",
    "        for k in range(0,len(zh_data[i][j])):\n",
    "            if not zh_data[i][j][k] in zh_data_dict:\n",
    "                zh_data_dict[zh_data[i][j][k]]=1\n",
    "            else:\n",
    "                zh_data_dict[zh_data[i][j][k]] = zh_data_dict[zh_data[i][j][k]]+1\n",
    "        source.append(zh_data[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始跑模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LOSS: 110875.07081022441\n",
      "EPOCH: 1 LOSS: 104323.88601961604\n",
      "EPOCH: 2 LOSS: 99568.40993533852\n",
      "EPOCH: 3 LOSS: 95863.33319794212\n",
      "EPOCH: 4 LOSS: 92913.88367548105\n"
     ]
    }
   ],
   "source": [
    "#--- EXAMPLE RUN --------------------------------------------------------------+\n",
    "\n",
    "settings = {}\n",
    "settings['n'] = 32                 # dimension of word embeddings\n",
    "settings['window_size'] = 5         # context window +/- center word\n",
    "settings['min_count'] = 0           # minimum word count\n",
    "settings['epochs'] = 5           # number of training epochs\n",
    "settings['neg_samp'] = 10           # number of negative words to use during training\n",
    "settings['learning_rate'] = 0.01    # learning rate\n",
    "np.random.seed(0)                   # set the seed for reproducibility\n",
    "\n",
    "#corpus = [['the','quick','brown','fox','jumped','over','the','lazy','dog']]\n",
    "corpus = source[:100]\n",
    "\n",
    "# INITIALIZE W2V MODEL\n",
    "w2v = word2vec()\n",
    "\n",
    "# generate training data\n",
    "training_data = w2v.generate_training_data(settings, corpus)\n",
    "\n",
    "# train word2vec model\n",
    "w2v.train(training_data)\n",
    "\n",
    "#--- END ----------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBow = Word2Vec(source,min_count=5,size=200,window=5,iter=50, workers=4,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhCBow = Word2Vec(source,min_count=5,size=200,window=5,iter=50, workers=5,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(w2v_model, words, topn=5):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>computer</th>\n",
       "      <th>cos</th>\n",
       "      <th>method</th>\n",
       "      <th>cos</th>\n",
       "      <th>learn</th>\n",
       "      <th>cos</th>\n",
       "      <th>paper</th>\n",
       "      <th>cos</th>\n",
       "      <th>machine</th>\n",
       "      <th>cos</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>cos</th>\n",
       "      <th>medical</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vision</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>technique</td>\n",
       "      <td>0.844628</td>\n",
       "      <td>start</td>\n",
       "      <td>0.788737</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.940712</td>\n",
       "      <td>learning</td>\n",
       "      <td>0.792323</td>\n",
       "      <td>scheme</td>\n",
       "      <td>0.892851</td>\n",
       "      <td>nursing</td>\n",
       "      <td>0.814213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bim</td>\n",
       "      <td>0.796203</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.844047</td>\n",
       "      <td>recognize</td>\n",
       "      <td>0.757144</td>\n",
       "      <td>dissertation</td>\n",
       "      <td>0.932019</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.742843</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.845073</td>\n",
       "      <td>emergency</td>\n",
       "      <td>0.808847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reality</td>\n",
       "      <td>0.774201</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>0.834030</td>\n",
       "      <td>select</td>\n",
       "      <td>0.756999</td>\n",
       "      <td>article</td>\n",
       "      <td>0.885475</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>method</td>\n",
       "      <td>0.834030</td>\n",
       "      <td>military</td>\n",
       "      <td>0.808737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science</td>\n",
       "      <td>0.766601</td>\n",
       "      <td>procedure</td>\n",
       "      <td>0.833115</td>\n",
       "      <td>protect</td>\n",
       "      <td>0.756079</td>\n",
       "      <td>work</td>\n",
       "      <td>0.846151</td>\n",
       "      <td>software</td>\n",
       "      <td>0.709588</td>\n",
       "      <td>procedure</td>\n",
       "      <td>0.819175</td>\n",
       "      <td>staff</td>\n",
       "      <td>0.799391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>large-scale</td>\n",
       "      <td>0.765390</td>\n",
       "      <td>scheme</td>\n",
       "      <td>0.795746</td>\n",
       "      <td>leverage</td>\n",
       "      <td>0.744813</td>\n",
       "      <td>essay</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>stereo</td>\n",
       "      <td>0.682782</td>\n",
       "      <td>framework</td>\n",
       "      <td>0.785970</td>\n",
       "      <td>care</td>\n",
       "      <td>0.796190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      computer       cos     method       cos      learn       cos  \\\n",
       "0       vision  0.832695  technique  0.844628      start  0.788737   \n",
       "1          bim  0.796203   approach  0.844047  recognize  0.757144   \n",
       "2      reality  0.774201  algorithm  0.834030     select  0.756999   \n",
       "3      science  0.766601  procedure  0.833115    protect  0.756079   \n",
       "4  large-scale  0.765390     scheme  0.795746   leverage  0.744813   \n",
       "\n",
       "          paper       cos   machine       cos  algorithm       cos    medical  \\\n",
       "0        thesis  0.940712  learning  0.792323     scheme  0.892851    nursing   \n",
       "1  dissertation  0.932019       svm  0.742843   approach  0.845073  emergency   \n",
       "2       article  0.885475    vector  0.728682     method  0.834030   military   \n",
       "3          work  0.846151  software  0.709588  procedure  0.819175      staff   \n",
       "4         essay  0.803591    stereo  0.682782  framework  0.785970       care   \n",
       "\n",
       "        cos  \n",
       "0  0.814213  \n",
       "1  0.808847  \n",
       "2  0.808737  \n",
       "3  0.799391  \n",
       "4  0.796190  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#只跑5個iteration\n",
    "most_similar(CBow, ['computer','method','learn','paper','machine','algorithm','medical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>computer</th>\n",
       "      <th>cos</th>\n",
       "      <th>method</th>\n",
       "      <th>cos</th>\n",
       "      <th>learn</th>\n",
       "      <th>cos</th>\n",
       "      <th>paper</th>\n",
       "      <th>cos</th>\n",
       "      <th>machine</th>\n",
       "      <th>cos</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>cos</th>\n",
       "      <th>medical</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computers</td>\n",
       "      <td>0.495371</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.807630</td>\n",
       "      <td>extract</td>\n",
       "      <td>0.483715</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.901730</td>\n",
       "      <td>machines</td>\n",
       "      <td>0.438179</td>\n",
       "      <td>method</td>\n",
       "      <td>0.734917</td>\n",
       "      <td>nursing</td>\n",
       "      <td>0.517037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kinesthetic</td>\n",
       "      <td>0.408801</td>\n",
       "      <td>technique</td>\n",
       "      <td>0.749219</td>\n",
       "      <td>see</td>\n",
       "      <td>0.470761</td>\n",
       "      <td>dissertation</td>\n",
       "      <td>0.775195</td>\n",
       "      <td>supervised</td>\n",
       "      <td>0.426178</td>\n",
       "      <td>scheme</td>\n",
       "      <td>0.701819</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>0.499971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphics</td>\n",
       "      <td>0.404766</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>0.734917</td>\n",
       "      <td>know</td>\n",
       "      <td>0.470506</td>\n",
       "      <td>study</td>\n",
       "      <td>0.668645</td>\n",
       "      <td>software</td>\n",
       "      <td>0.361836</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.699059</td>\n",
       "      <td>clinic</td>\n",
       "      <td>0.469472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>software</td>\n",
       "      <td>0.390050</td>\n",
       "      <td>scheme</td>\n",
       "      <td>0.697332</td>\n",
       "      <td>create</td>\n",
       "      <td>0.466998</td>\n",
       "      <td>article</td>\n",
       "      <td>0.649461</td>\n",
       "      <td>cnc</td>\n",
       "      <td>0.360410</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>0.665026</td>\n",
       "      <td>hospital</td>\n",
       "      <td>0.457505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nvidia</td>\n",
       "      <td>0.386236</td>\n",
       "      <td>methods</td>\n",
       "      <td>0.654628</td>\n",
       "      <td>construct</td>\n",
       "      <td>0.464892</td>\n",
       "      <td>research</td>\n",
       "      <td>0.642569</td>\n",
       "      <td>aoi</td>\n",
       "      <td>0.357666</td>\n",
       "      <td>technique</td>\n",
       "      <td>0.597952</td>\n",
       "      <td>health</td>\n",
       "      <td>0.452118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      computer       cos     method       cos      learn       cos  \\\n",
       "0    computers  0.495371   approach  0.807630    extract  0.483715   \n",
       "1  kinesthetic  0.408801  technique  0.749219        see  0.470761   \n",
       "2     graphics  0.404766  algorithm  0.734917       know  0.470506   \n",
       "3     software  0.390050     scheme  0.697332     create  0.466998   \n",
       "4       nvidia  0.386236    methods  0.654628  construct  0.464892   \n",
       "\n",
       "          paper       cos     machine       cos   algorithm       cos  \\\n",
       "0        thesis  0.901730    machines  0.438179      method  0.734917   \n",
       "1  dissertation  0.775195  supervised  0.426178      scheme  0.701819   \n",
       "2         study  0.668645    software  0.361836    approach  0.699059   \n",
       "3       article  0.649461         cnc  0.360410  algorithms  0.665026   \n",
       "4      research  0.642569         aoi  0.357666   technique  0.597952   \n",
       "\n",
       "      medical       cos  \n",
       "0     nursing  0.517037  \n",
       "1  healthcare  0.499971  \n",
       "2      clinic  0.469472  \n",
       "3    hospital  0.457505  \n",
       "4      health  0.452118  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(CBow, ['computer','method','learn','paper','machine','algorithm','medical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(zhCBow, ['電腦','方法','學習','論文','機器','演算法','醫學'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBow.save('CBzh0505.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
